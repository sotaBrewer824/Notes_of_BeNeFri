# 📘 贝叶斯强化学习（Bayesian RL）复习笔记（基于 Lab10）

## ✅ 基本思想

贝叶斯强化学习的目标是：  
**不仅估计一个值函数或 Q 值，而是为每个 Q 值维护一个概率分布**，从而表达我们对其的“不确定性”。

这使得智能体能够：
- 主动探索不确定区域
- 使用采样（如 Thompson Sampling）来平衡探索与利用

---

## 🎯 在 Lab10 中的设置

我们对每个状态-动作对 (s, a) 维护：

Q(s, a) ~ N(μ_{s,a}, σ²_{s,a})

更新时我们用的是**贝叶斯推理（conjugate prior）**  
- 用 TD 目标作为观测
- 假设观测噪声为高斯：  
  ε ~ N(0, σ²_ε)

---

## 🔁 更新公式（高斯先验 + 高斯观测）

设：
- 当前估计：μ_{s,a}, σ²_{s,a}
- 新观测（TD目标）：y = r + γ max_{a'} Q(s', a') + ε

则更新后：

- 均值更新：
  μ' = (σ²_ε * μ + σ² * y) / (σ² + σ²_ε)

- 方差更新：
  σ'² = (σ² * σ²_ε) / (σ² + σ²_ε)

---

## 🧠 动作选择策略：Thompson Sampling

对于每一个状态 s，执行：

1. 从每个 Q(s, a) ~ N(μ, σ²) 中采样
2. 选择最大值对应的动作：
   a_t = argmax_a Q_sample(s, a)

✅ 这种做法天然支持探索（高不确定性会更可能被尝试）

---

## 🧪 实验过程回顾（Lab10）

1. 初始化所有 Q(s,a) ~ N(0, 1)
2. 在环境中进行交互，得到 s, a, r, s'
3. 使用 TD 目标 + 采样，更新每个 Q(s, a)
4. 使用 Thompson Sampling 选择动作

---

## ✅ 优点总结

- 支持**不确定性驱动的探索**
- 高效利用有限样本
- 与 Multi-armed Bandit 的贝叶斯思想类似

---

## ⚠️ 缺点总结

- 更新与推理计算较慢（涉及采样与分布）
- 难以扩展到高维（如深度强化学习场景）

---

## 📌 总结对比（与传统 Q-learning）

| 特性            | Q-learning        | Bayesian Q-learning      |
|-----------------|-------------------|---------------------------|
| Q值表示         | 单个点估计        | 一个高斯分布             |
| 探索机制        | ε-Greedy / UCB    | Thompson Sampling（采样）|
| 是否量化不确定性 | 否                | 是                        |
| 是否支持主动探索 | 否                | 是                        |
| 算法复杂度      | 低                | 中（需更新分布）         |

---
