# 📘 UCRL（Upper Confidence Reinforcement Learning）复习笔记（基于 Lab11）

## ✅ 基本思想

UCRL 是一种基于置信区间的强化学习方法，其核心策略是：

> “我不知道环境的真实模型，但我可以构造一个置信区间包含真实模型的集合，然后假设这个集合中最乐观的那个是真的。”

这种思想被称为 **乐观探索（Optimism in the Face of Uncertainty, OFU）**。

---

## 🧪 Lab11 实验流程回顾

1. 给定一个离散的 MDP（例如 gym 的 FrozenLake 环境）
2. 收集经验数据统计：
   - 每个状态动作对 (s, a) 的访问次数 N_t(s, a)
   - 收到的总奖励 R_t(s, a)
   - 转移到下一个状态 s' 的次数 P_t(s, a, s')
3. 用这些统计量估计：
   - r̂_t(s, a) = R_t(s, a) / N_t(s, a)
   - p̂_t(s'|s, a) = P_t(s, a, s') / N_t(s, a)

---

## 🧾 UCRL 的关键公式

### 1. 奖励置信区间：
conf_r(t,s,a) = min { 1, sqrt( log(2 t^α |S||A|) / (2 N_t(s,a)) ) }

### 2. 转移概率置信区间：
conf_p(t,s,a) = min { 1, sqrt( log(4 t^α |S|^2 |A|) / (2 N_t(s,a)) ) }

这些界限表示在第 t 步，我们可以置信地认为真实的 r(s,a)、p(s'|s,a) 落在这个区间中。

---

## 🔁 策略更新步骤

1. 根据经验估计计算 reward / transition 的置信上界
2. 构造“最乐观”的 MDP（在置信区间内选择 reward 最大、转移最有利的模型）
3. 用 Value Iteration 解出当前的最优策略
4. 执行该策略一段时间（直到模型变化足够多）
5. 重新收集数据，更新估计，重复步骤

---

## ✅ UCRL 的优点

- ✅ 探索–利用的平衡方式合理且有理论保障
- ✅ 在表格 MDP 中可以达到 PAC-optimal（可能近似最优）
- ✅ 不需要手动调节 ε 或采样方法

---

## ⚠️ UCRL 的限制

- ❌ 只能用于有限状态动作空间的表格型 MDP
- ❌ 每次计算策略时要解整个模型，计算成本较高
- ❌ 不适合高维状态（如图像、文本等）

---

## 📌 小结

| 特性           | UCRL                          |
|----------------|-------------------------------|
| 是否建模环境   | ✅ 显式建模                   |
| 不确定性建模   | ✅ 置信区间                   |
| 探索机制       | 乐观估计（最有利的模型）     |
| 是否有理论保障 | ✅ PAC optimal                |
| 适合环境       | 小规模、表格型 MDP           |
| 难点           | 不适合高维、计算代价高       |

---
