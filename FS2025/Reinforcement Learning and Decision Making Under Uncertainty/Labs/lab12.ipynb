{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "584c6942",
   "metadata": {},
   "source": [
    "## RLDMUU 2025\n",
    "#### MARL - introduction\n",
    "jakub.tluczek@unine.ch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62401b8f",
   "metadata": {},
   "source": [
    "In the last lab of the RLDMUU course, we are going to explore the basics of multi agent reinforcement learning. We are going to go through the Petting Zoo workflow (Gymnasium-based framework for multi agent setting), as well as an example end to end training of multiple agents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1155ab8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pettingzoo\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from collections import deque\n",
    "import random\n",
    "torch.manual_seed(123)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "np.random.seed(123)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2967574f",
   "metadata": {},
   "source": [
    "Petting Zoo offer the same environment based workflow as Gymnasium, while offering two APIs for agent interaction:\n",
    "\n",
    "- Agent Environment Cycle (AEC) API - turn based\n",
    "- Parallel API - moves are made simultaneously\n",
    "\n",
    "Petting zoo features similar `step` function we know from Gymnasium, with this difference however, that here we operate on dictionaries, where each agent's observation, reward or action is an entry in the `dict` with agent's identifier serving as a key. Moreover, in AEC API `step` only acts on the environment, while the observations are retrieved using `last` method. Let's take a look at an example, a classic tic-tac-toe game with AEC:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72ef15c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pettingzoo.classic import tictactoe_v3\n",
    "\n",
    "env = tictactoe_v3.env()\n",
    "env.reset()\n",
    "\n",
    "# agent_iter is an iterable object that generates an ID of the agent whose turn is it right now\n",
    "for agent in env.agent_iter():\n",
    "    observation, reward, termination, truncation, info = env.last()\n",
    "\n",
    "    if termination or truncation:\n",
    "        action = None\n",
    "    else:\n",
    "        # action mask part in the observation vector tells us which actions are allowed\n",
    "        mask = observation[\"action_mask\"]\n",
    "\n",
    "        action = env.action_space(agent).sample(mask)\n",
    "\n",
    "    env.step(action)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73d05ab0",
   "metadata": {},
   "source": [
    "Another way to act with multiagent environment is to act in parallel. Agents take action at the same time, without observing the current action of other agents. Observations are retrieved in a similar way as in Gymnasium:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9719af3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pettingzoo.butterfly import knights_archers_zombies_v10\n",
    "\n",
    "env = knights_archers_zombies_v10.parallel_env()\n",
    "observations, infos = env.reset()\n",
    "\n",
    "# instead of generating current IDs from iterable, loop until therea are no more active agents\n",
    "while env.agents:\n",
    "    # sampling actions for all agents\n",
    "    actions = {agent: env.action_space(agent).sample() for agent in env.agents}\n",
    "\n",
    "    observations, rewards, terminations, truncations, infos = env.step(actions)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61d1fd85",
   "metadata": {},
   "source": [
    "Let's stick to tic-tac-toe for the purpose of this class. The nice thing about classical games is that they have nice, predefined goal (winning) so we don't have to think about the solution concepts. In case of tic tac toe we also assume that it's not in agent's interest to cooperate, as it is ultimately a zero sum game. Though simple, this environment still suffers from usual problems related to MARL - implementing a simple Q-Learning is tricky, since the environment is not stationary, moreover the rewards are incredibly sparse (they are only awarded at the end of the game). \n",
    "\n",
    "Let's take a look at the first method, called **self play**. Here we play a game against our own policy, which we update periodically. As we improve our $\\pi$, after each time we update the \"opponent\", it gets better at the game. First, let's reuse the DQN we have coded before, it will form the basis of our policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23d9f23a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QNetwork(nn.Module):\n",
    "    def __init__(self, n_states, n_actions, hidden_dim):\n",
    "        super(QNetwork, self).__init__()\n",
    "        self.linear1 = nn.Linear(n_states, hidden_dim)\n",
    "        self.linear2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.linear3 = nn.Linear(hidden_dim, n_actions)\n",
    "\n",
    "    def forward(self, state):\n",
    "        x = F.relu(self.linear1(state))\n",
    "        x = F.relu(self.linear2(x))\n",
    "        return self.linear3(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfd9f0b6",
   "metadata": {},
   "source": [
    "At first, agent doesn't know the game at all, so you can initially fill the replay buffer with random games - just sample the action spaces of both players. Remember to use action mask!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1c1fab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self, size):\n",
    "        self.memory = deque([], maxlen=size)\n",
    "\n",
    "    # this method samples transitions and returns tensors of each type registered in the environment step\n",
    "    def sample(self, sample_size):\n",
    "        sample = random.sample(self.memory, sample_size)\n",
    "        states = []\n",
    "        actions = []\n",
    "        rewards = []\n",
    "        next_states = []\n",
    "        dones = []\n",
    "        for x in sample:\n",
    "            states.append(x[0])\n",
    "            actions.append(x[1])\n",
    "            rewards.append(x[2])\n",
    "            next_states.append(x[3])\n",
    "            dones.append(x[4])\n",
    "        states = torch.tensor(states).to(device)\n",
    "        actions = torch.tensor(actions).to(device)\n",
    "        rewards = torch.tensor(rewards).to(device)\n",
    "        next_states = torch.tensor(next_states).to(device)\n",
    "        dones = torch.tensor(dones, dtype=torch.int).to(device)\n",
    "        return states, actions, rewards, next_states, dones\n",
    "    \n",
    "    # add transition to the buffer\n",
    "    def append(self, item):\n",
    "        self.memory.append(item)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "    \n",
    "def parameter_update(source_model, target_model, tau):\n",
    "    for target_param, source_param in zip(target_model.parameters(), source_model.parameters()):\n",
    "        target_param.data.copy_(tau * source_param.data + (1.0 - tau)*target_param.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ed10fff",
   "metadata": {},
   "source": [
    "Now your task is to define a policy of an agent we want to train, as well as \"oponnent\" policy. You can reuse the code from Lab 8. Although you can reuse the target network as the \"oponnent\", remember to turn off gradient updates when picking an oponnent's move. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89890541",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 超参数设置\n",
    "hidden_dim = 64\n",
    "replay_buffer_size = 10000\n",
    "batch_size = 64\n",
    "gamma = 0.99\n",
    "lr = 1e-3\n",
    "epsilon = 1.0\n",
    "epsilon_end = 0.1\n",
    "epsilon_decay = 0.995\n",
    "tau = 0.01\n",
    "\n",
    "# 初始化环境和网络\n",
    "env = tictactoe_v3.env()\n",
    "env.reset()\n",
    "obs_shape = env.observation_spaces[env.agents[0]]['observation'].shape[0]\n",
    "n_actions = env.action_spaces[env.agents[0]].n\n",
    "\n",
    "# 共享 Q 网络\n",
    "q_net = QNetwork(obs_shape, n_actions, hidden_dim).to(device)\n",
    "target_q_net = QNetwork(obs_shape, n_actions, hidden_dim).to(device)\n",
    "target_q_net.load_state_dict(q_net.state_dict())\n",
    "\n",
    "optimizer = torch.optim.Adam(q_net.parameters(), lr=lr)\n",
    "replay_buffer = ReplayBuffer(replay_buffer_size)\n",
    "\n",
    "# 用随机策略填充经验池\n",
    "print(\"Filling replay buffer with random transitions...\")\n",
    "while len(replay_buffer) < 1000:\n",
    "    env.reset()\n",
    "    for agent in env.agent_iter():\n",
    "        obs, reward, term, trunc, info = env.last()\n",
    "        if term or trunc:\n",
    "            action = None\n",
    "        else:\n",
    "            mask = obs[\"action_mask\"]\n",
    "            valid_actions = np.flatnonzero(mask)\n",
    "            action = np.random.choice(valid_actions)\n",
    "        next_obs, next_reward, next_term, next_trunc, _ = obs, reward, term, trunc, info\n",
    "        env.step(action)\n",
    "        if action is not None:\n",
    "            replay_buffer.append((obs[\"observation\"], action, reward, next_obs[\"observation\"], term or trunc))\n",
    "\n",
    "print(\"Replay buffer filled.\")\n",
    "\n",
    "# DQN 训练循环\n",
    "NUM_TRAJECTORIES = 1000\n",
    "for episode in range(NUM_TRAJECTORIES):\n",
    "    env.reset()\n",
    "    for agent in env.agent_iter():\n",
    "        obs, reward, term, trunc, info = env.last()\n",
    "\n",
    "        if term or trunc:\n",
    "            env.step(None)\n",
    "            continue\n",
    "\n",
    "        # epsilon-greedy 动作选择\n",
    "        obs_tensor = torch.tensor(obs[\"observation\"], dtype=torch.float32).unsqueeze(0).to(device)\n",
    "        mask = torch.tensor(obs[\"action_mask\"], dtype=torch.bool).to(device)\n",
    "        if random.random() < epsilon:\n",
    "            action = np.random.choice(np.flatnonzero(obs[\"action_mask\"]))\n",
    "        else:\n",
    "            q_values = q_net(obs_tensor)\n",
    "            q_values[~mask] = -float('inf')\n",
    "            action = torch.argmax(q_values).item()\n",
    "\n",
    "        # 执行动作\n",
    "        env.step(action)\n",
    "        next_obs, next_reward, next_term, next_trunc, _ = env.last()\n",
    "        done = next_term or next_trunc\n",
    "        if not done:\n",
    "            replay_buffer.append((obs[\"observation\"], action, reward, next_obs[\"observation\"], done))\n",
    "\n",
    "        # 学习\n",
    "        if len(replay_buffer) >= batch_size:\n",
    "            states, actions, rewards, next_states, dones = replay_buffer.sample(batch_size)\n",
    "\n",
    "            q_values = q_net(states.float()).gather(1, actions.unsqueeze(1)).squeeze(1)\n",
    "            next_q_values = target_q_net(next_states.float()).max(1)[0]\n",
    "            targets = rewards + gamma * next_q_values * (1 - dones)\n",
    "\n",
    "            loss = F.mse_loss(q_values, targets.detach())\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # 软更新 target network\n",
    "            parameter_update(q_net, target_q_net, tau)\n",
    "\n",
    "    # 衰减 epsilon\n",
    "    epsilon = max(epsilon_end, epsilon * epsilon_decay)\n",
    "\n",
    "    if episode % 100 == 0:\n",
    "        print(f\"Episode {episode}, Epsilon: {epsilon:.3f}\")\n",
    "\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63a071d5",
   "metadata": {},
   "source": [
    "Finally your task would be to create a training loop for DQN with self-play. Plot the results after you are finished."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee7266f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_TRAJECTORIES = 1000\n",
    "\n",
    "env = tictactoe_v3.env()\n",
    "env.reset()\n",
    "\n",
    "# TODO: fill the buffer with the random transitions\n",
    "\n",
    "# TODO: create an AEC training loop\n",
    "# Hint : You can reuse the DQN training code from Lab 8 \n",
    "# Remember however about action masking and specific way multi agent environment is updated\n",
    "\n",
    "for tau in range(NUM_TRAJECTORIES):\n",
    "    pass\n",
    "\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42ec7e17",
   "metadata": {},
   "source": [
    "*BONUS*: Since the rewards are very sparse, you can augment the reward function and use another technique called **curriculum learning**. Using your expert knowledge you can then lead policy into regions that are known to work in the particular game. For example, try awarding agent for putting 2 X's or O's in row without opponents piece."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
