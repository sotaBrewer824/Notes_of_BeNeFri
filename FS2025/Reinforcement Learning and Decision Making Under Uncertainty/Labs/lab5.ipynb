{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "upWDgG2rix09"
      },
      "source": [
        "## RLDMUU 2025\n",
        "#### Policy Iteration\n",
        "jakub.tluczek@unine.ch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "8vp7aXtWix0-"
      },
      "outputs": [],
      "source": [
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class DiscreteMDP:\n",
        "    def __init__(self, n_states, n_actions, P = None, R = None):\n",
        "        self.n_states = n_states\n",
        "        self.n_actions = n_actions\n",
        "        if (P is None):\n",
        "            self.P = np.zeros([n_states, n_actions, n_states])\n",
        "            for s in range(self.n_states):\n",
        "                for a in range(self.n_actions):\n",
        "                    self.P[s,a] = np.random.dirichlet(np.ones(n_states))\n",
        "        else:\n",
        "            self.P = P\n",
        "        if (R is None):\n",
        "            self.R = np.zeros([n_states, n_actions])\n",
        "            for s in range(self.n_states):\n",
        "                for a in range(self.n_actions):\n",
        "                    self.R[s,a] = np.round(np.random.uniform(), decimals=1)\n",
        "        else:\n",
        "            self.R = R\n",
        "\n",
        "        for s in range(self.n_states):\n",
        "            for a in range(self.n_actions):\n",
        "                assert(abs(np.sum(self.P[s,a,:])-1) <= 1e-3)\n",
        "                assert((self.P[s,a,:] <= 1).all())\n",
        "                assert((self.P[s,a,:] >= 0).all())\n",
        "\n",
        "    def get_transition_probability(self, state, action, next_state):\n",
        "        return self.P[state, action, next_state]\n",
        "\n",
        "    def get_transition_probabilities(self, state, action):\n",
        "        return self.P[state, action]\n",
        "\n",
        "    def get_reward(self, state, action):\n",
        "        return self.R[state, action]"
      ],
      "metadata": {
        "id": "0AhJr1H8uPTB"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Icl1L_2Tix0_"
      },
      "outputs": [],
      "source": [
        "class ChainMDP(DiscreteMDP):\n",
        "    \"\"\"\n",
        "    Problem where we need to take the same action n_states-1 time in a row to get a highly rewarding state\n",
        "    The optimal policy greatly depends on the discount factor we choose.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, n_states=20):\n",
        "        assert  n_states > 1\n",
        "\n",
        "        n_actions = 2\n",
        "        super().__init__(n_states=n_states, n_actions=n_actions)\n",
        "\n",
        "        self.R[:] = 0.\n",
        "        self.P[:] = 0.\n",
        "\n",
        "        self.R[:, 1] = -1 / (n_states-1)\n",
        "        self.R[n_states-1, 1] = 1.\n",
        "        self.R[:, 0] = 1/n_states\n",
        "\n",
        "        for i in range(self.n_states-1):\n",
        "            if i > 0:\n",
        "                self.P[i, 0, i-1] = 1.\n",
        "            else:\n",
        "                self.P[i, 0, i] = 1.\n",
        "\n",
        "            self.P[i, 1, i+1] = 1.\n",
        "\n",
        "        self.P[self.n_states-1, :, self.n_states-1] = 1."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xrWOtneVix0_"
      },
      "source": [
        "### Policy iteration"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BhtVGvLxix0_"
      },
      "source": [
        "Your first task will be to implement the policy iteration algorithm. Let's start with policy evaluation. Given a policy $\\pi$, you have to evaluate this policy on all states.\n",
        "\n",
        "$$ V^{\\pi}(s) = \\sum_{s'} P(s' | s, \\pi(s)) [R(s, \\pi(s)) + \\gamma V^{\\pi}(s')] $$\n",
        "\n",
        "You can either program the policy evaluation using dynamic programming, or by using the equation:\n",
        "\n",
        "$$ V^{\\pi} = \\left[\\mathbb{I} - \\gamma \\mathbb{P} \\right]^{-1} r $$\n",
        "\n",
        "where $\\mathbb{I}$ is an identity matrix, $\\mathbb{P}$ a probability matrix and $r$ a reward vector."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "JklEFaroix0_"
      },
      "outputs": [],
      "source": [
        "def policy_evaluation_dynamic_programming(mdp: DiscreteMDP, gamma: float, policy: list[int], n_iters: int):\n",
        "    # TODO: Implement policy evaluation using dynamic programming\n",
        "    V = 0\n",
        "    for i in range(n_iters):\n",
        "        V_new = np.zeros(mdp.n_states)\n",
        "        for s in range(mdp.n_states):\n",
        "            a = policy[s]\n",
        "            P = mdp.get_transition_probabilities(s, a)\n",
        "            R = mdp.get_reward(s, a)\n",
        "            V_new[s] += np.dot(P, (R + gamma * V))\n",
        "            V = V_new\n",
        "    return V\n",
        "\n",
        "\n",
        "def policy_evaluation_matrix_multiplication(mdp: DiscreteMDP, gamma: float, policy: list[int]):\n",
        "    # TODO: Implement policy evaluation using matrix operations\n",
        "    P = np.zeros((mdp.n_states, mdp.n_states))\n",
        "    r = np.zeros(mdp.n_states)\n",
        "    for s in range(mdp.n_states):\n",
        "        a = policy[s]\n",
        "        P[s] = mdp.get_transition_probabilities(s, a)\n",
        "        r[s] = mdp.get_reward(s, a)\n",
        "    V = np.linalg.inv(np.eye(mdp.n_states) - gamma * P) @ r\n",
        "    return V"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GJhqR4gZix1A"
      },
      "source": [
        "Now implement policy iteration by evaluating policy at each state and set the policy to:\n",
        "\n",
        "$$ \\pi(s) = \\arg\\max_{a \\in A} Q^\\pi (s,a) $$\n",
        "\n",
        "Iterate until you reach maximal number of iterations or until newly computed values don't differ by more than some $\\epsilon$ or until $\\pi$ doesn't change."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "F_hItwlFix1A"
      },
      "outputs": [],
      "source": [
        "def policy_iteration(mdp: DiscreteMDP, gamma: float, n_iters: int, n_eval_iters: int, use_dp: bool = False):\n",
        "    policy = np.zeros(mdp.n_states, dtype=int)\n",
        "    for i in range(n_iters):\n",
        "        if use_dp:\n",
        "            V = policy_evaluation_dynamic_programming(mdp, gamma, policy, n_eval_iters)\n",
        "        else:\n",
        "            V = policy_evaluation_matrix_multiplication(mdp, gamma, policy)\n",
        "\n",
        "        flag = True\n",
        "        for s in range(mdp.n_states):\n",
        "            old_action = policy[s]\n",
        "            Q_values = np.zeros(mdp.n_actions)\n",
        "            for a in range(mdp.n_actions):\n",
        "                for s_next in range(mdp.n_states):\n",
        "                    P = mdp.get_transition_probability(s, a, s_next)\n",
        "                    R = mdp.get_reward(s, a)\n",
        "                    V_next = V[s_next]\n",
        "                    Q_values[a] += P * (R + gamma * V_next)\n",
        "            best_action = np.argmax(Q_values)\n",
        "            policy[s] = best_action\n",
        "\n",
        "            if old_action != best_action:\n",
        "                flag = False\n",
        "        if flag:\n",
        "            break\n",
        "    return policy, V"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h3mH0MjHix1A"
      },
      "source": [
        "### Value iteration"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KlV_EcYgix1A"
      },
      "source": [
        "Your second task will be to implement Value Iteration algorithm. At each timestep $t$, you have to compute a new value function by maximizing the Bellman equation.\n",
        "\n",
        "$$ V_t (s) = \\max_a \\left( \\sum_{s'} P(s'|s, a) [R(s,a) + \\gamma V_{t-1}(s')] \\right) $$\n",
        "\n",
        "Once $V$ converges to some $V^{*}$ (or once you reach the limit of iterations), you can extract the policy for each state:\n",
        "\n",
        "$$ \\pi^{*}(s) = \\arg\\max_a \\sum_{s'} P(s' | s, a) [R(s,a) + \\gamma V^{*}(s')] $$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "NqGcUagIix1A"
      },
      "outputs": [],
      "source": [
        "def value_iteration(mdp: DiscreteMDP, gamma: float, n_iters: int):\n",
        "    policy = np.zeros(mdp.n_states)\n",
        "    V = np.zeros(mdp.n_states)\n",
        "    Q = np.zeros([mdp.n_states, mdp.n_actions])\n",
        "    for _ in range(n_iters):\n",
        "        for s in range(mdp.n_states):\n",
        "            for a in range(mdp.n_actions):\n",
        "                Q[s,a] = mdp.get_reward(s,a)\n",
        "                P = mdp.get_transition_probabilities(s,a)\n",
        "                for next_s in range(mdp.n_states):\n",
        "                    Q[s,a] += gamma * P[next_s] * V[next_s]\n",
        "        V = np.max(Q, axis=1)\n",
        "        policy = np.argmax(Q, axis=1)\n",
        "    return policy, V\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SccLehO9ix1A"
      },
      "source": [
        "### Results\n",
        "\n",
        "Run both methods on the provided MDP for a given Chain MDP instance and compare the results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q7nvaGbqix1A",
        "outputId": "b533b77e-299e-4152-987d-c825e203a1c2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "POLICY ITERATION\n",
            "POLICY:\n",
            "[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
            "V\n",
            "[ 0.89563339  1.05362774  1.22917702  1.42423178  1.64095929  1.88176763\n",
            "  2.14933245  2.4466267   2.77695364  3.14398358  3.55179462  4.004918\n",
            "  4.50838842  5.0678      5.68936842  6.38        7.14736842  8.\n",
            "  8.94736842 10.        ]\n",
            "VALUE ITERATION\n",
            "POLICY:\n",
            "[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
            "V\n",
            "[ 0.89563339  1.05362774  1.22917702  1.42423178  1.64095929  1.88176763\n",
            "  2.14933245  2.4466267   2.77695364  3.14398358  3.55179462  4.004918\n",
            "  4.50838842  5.0678      5.68936842  6.38        7.14736842  8.\n",
            "  8.94736842 10.        ]\n"
          ]
        }
      ],
      "source": [
        "mdp = ChainMDP()\n",
        "\n",
        "N_ITERS = 10_000\n",
        "N_EVAL_ITERS = 100\n",
        "GAMMA = .9\n",
        "\n",
        "policy, V = policy_iteration(mdp, GAMMA, N_ITERS, N_EVAL_ITERS)\n",
        "print(\"POLICY ITERATION\")\n",
        "print(f\"POLICY:\\n{policy}\")\n",
        "print(f\"V\\n{V}\")\n",
        "policy, V = value_iteration(mdp, GAMMA, N_ITERS)\n",
        "print(\"VALUE ITERATION\")\n",
        "print(f\"POLICY:\\n{policy}\")\n",
        "print(f\"V\\n{V}\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "py10",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.16"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}