{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "43sqj-lli8u-"
      },
      "source": [
        "## RLDMUU 2025\n",
        "#### Backward Induction\n",
        "jakub.tluczek@unine.ch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6UCNzHiwi8vA"
      },
      "source": [
        "Today your task would be to implement the backwards induction algorithm for the following MDP (you can also find it in `src/MDP/MDP.py` on our github):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "UoZNBW0Ki8vB"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "## This a discrete MDP with a finite number of states and actions\n",
        "class DiscreteMDP:\n",
        "    ## initalise a random MDP with\n",
        "    ## n_states: the number of states\n",
        "    ## n_actions: the number of actions\n",
        "    ## Optional arguments:\n",
        "    ## P: the state-action-state transition matrix so that P[s,a,s_next] is the probability of s_next given the current state-action pair (s,a)\n",
        "    ## R: The state-action reward matrix so that R[s,a] is the reward for taking action a in state s.\n",
        "    def __init__(self, n_states, n_actions, P = None, R = None):\n",
        "        self.n_states = n_states # the number of states of the MDP\n",
        "        self.n_actions = n_actions # the number of actions of the MDP\n",
        "        if (P is None):\n",
        "            self.P = np.zeros([n_states, n_actions, n_states]) # the transition probability matrix of the MDP so that P[s,a,s'] is the probabiltiy of going to s' from (s,a)\n",
        "            for s in range(self.n_states):\n",
        "                for a in range(self.n_actions):\n",
        "                    self.P[s,a] = np.random.dirichlet(np.ones(n_states)) # generalisation of Beta to multiple outcome\n",
        "        else:\n",
        "            self.P = P\n",
        "        if (R is None):\n",
        "            self.R = np.zeros([n_states, n_actions]) # the expected reward for each action and state\n",
        "            # generate uniformly random transitions and 0.1 bernoulli rewards\n",
        "            for s in range(self.n_states):\n",
        "                for a in range(self.n_actions):\n",
        "                    self.R[s,a] = np.round(np.random.uniform(), decimals=1)\n",
        "        else:\n",
        "            self.R = R\n",
        "\n",
        "        # check transitions\n",
        "        for s in range(self.n_states):\n",
        "            for a in range(self.n_actions):\n",
        "                #print(s,a, \":\", self.P[s,a,:])\n",
        "                assert(abs(np.sum(self.P[s,a,:])-1) <= 1e-3)\n",
        "                assert((self.P[s,a,:] <= 1).all())\n",
        "                assert((self.P[s,a,:] >= 0).all())\n",
        "\n",
        "    # get the probability of next state j given current state s, action a, i.e. P(j|s,a)\n",
        "    def get_transition_probability(self, state, action, next_state):\n",
        "        return self.P[state, action, next_state]\n",
        "\n",
        "    # get the vector of probabilities over next states P( . | s,a)\n",
        "    def get_transition_probabilities(self, state, action):\n",
        "        return self.P[state, action]\n",
        "\n",
        "    # Get the reward for the current state action.\n",
        "    # It can also be interpreted as the expected reward for the state and action.\n",
        "    def get_reward(self, state, action):\n",
        "        return self.R[state, action]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mwmnm-u4i8vC"
      },
      "source": [
        "### Backward induction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SuPFBZH1i8vC"
      },
      "source": [
        "As a reminder, in the backward induction algorithm we consider an MDP with finite horizon $T$, and for each step of the algorithm, we compute:\n",
        "\n",
        "$$ V_t(s) = max_{a \\in A} \\left[ R(s,a) + \\sum_{s' \\in S} P(s' | s,a)V_{t+1}(s') \\right]$$\n",
        "\n",
        "where $R(s,a)$ is a reward received by picking action $a$ in state $s$, $P(s'|s,a)$ is the probability of transitioning into next state $s'$, and $V_{t+1}(s')$ is the value of said next state at time $t+1$. We can also say, that for the last timestep (with index $T-1$) the next state value is 0 for every state $V_T(s) = 0$. Consecutively, the action $a$ which maximizes $V_t(s)$ can be described as policy $\\pi_t(s)$ at state $s$ and time $t$,"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7uWs0Teli8vC"
      },
      "source": [
        "Your task is to implement this algorithm. Remember to do the inverse iteration, and iterate from $T-1$ to $0$, not the other way around. Your function should return matrix of state values for each $s$ and $t$, as well as resulting policy:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DQ8pOfv9i8vC"
      },
      "outputs": [],
      "source": [
        "# TODO: Implement backwards induction\n",
        "def backwards_induction(mdp, T):\n",
        "    V = np.zeros([mdp.n_states, T])\n",
        "    policy = np.zeros([mdp.n_states, T])\n",
        "    for t in range(T-1, -1, -1):\n",
        "        for s in range(mdp.n_states):\n",
        "            max_value = -np.inf\n",
        "            for a in range(mdp.n_actions):\n",
        "                value = mdp.get_reward(s, a)\n",
        "                for s_next in range(mdp.n_states):\n",
        "                    # Check if t+1 is within bounds before accessing V\n",
        "                    if t + 1 < T:\n",
        "                        value += mdp.get_transition_probability(s, a, s_next) * V[s_next, t+1]\n",
        "                if value > max_value:\n",
        "                    max_value = value\n",
        "                    policy[s, t] = a\n",
        "            V[s, t] = max_value\n",
        "    return V, policy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Fhz1kR8i8vD",
        "outputId": "d0996fc6-ca28-4ef3-da19-2604ff63d03e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[11.3398275  10.60101919  9.86221093  9.12340354  8.38459643  7.64578368\n",
            "   6.90695493  6.16817261  5.42954119  4.69088384  3.95025256  3.20898284\n",
            "   2.47862885  1.78817336  1.        ]\n",
            " [11.02017955 10.28137138  9.54256276  8.80375357  8.06494757  7.32615132\n",
            "   6.58733932  5.84841954  5.10948717  4.37152142  3.63495566  2.89180343\n",
            "   2.12641368  1.38933365  0.8       ]\n",
            " [11.49707933 10.75827111 10.01946323  9.28065537  8.54184503  7.80302947\n",
            "   7.06423393  6.32549703  5.58671238  4.84717222  4.10753511  3.37289306\n",
            "   2.65132597  1.88614047  1.        ]\n",
            " [10.57822696  9.83941884  9.10061072  8.36180178  7.62299142  6.88418746\n",
            "   6.14540218  5.40659441  4.66755838  3.92850227  3.19116998  2.45735747\n",
            "   1.70989459  0.92224429  0.2       ]\n",
            " [11.23903579 10.50022752  9.76141914  9.02261101  8.28380485  7.54499726\n",
            "   6.8061759   6.06732946  5.32863245  4.59017464  3.85132058  3.1078059\n",
            "   2.36766648  1.6538563   1.        ]]\n",
            "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0.]\n",
            " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]]\n"
          ]
        }
      ],
      "source": [
        "STATES = 5\n",
        "ACTIONS = 3\n",
        "\n",
        "T = 15\n",
        "\n",
        "mdp = DiscreteMDP(STATES, ACTIONS)\n",
        "\n",
        "V, policy = backwards_induction(mdp, T)\n",
        "\n",
        "print(V)\n",
        "\n",
        "print(policy)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
